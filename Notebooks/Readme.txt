1. Every one's script code work is under his/her name folder    
2. The 3 python files under Notebook folder is our best result notebooks  
3. The following is the summary of our work.

The following link is our final project notebooks:  
1) Video part best result: 
https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/visual_pre-trained_model.ipynb  
The notebook uses the scripts:  
https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Scripts/Patrick_Jean-Baptiste/get_data.py  
https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Scripts/Patrick_Jean-Baptiste/match_files.py

2) Audio part best result: https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/audio_song_speech.ipynb  

Our models:  
1) Video parts:   
video preprocessing:https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/video_preprocessing.ipynb   

face detection:https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/face_detection.ipynb  

face extraction:https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/face_extraction.ipynb  

emotion detection model:https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/visual_pre-trained_model.ipynb  

2) Audio part notebooks:   
Song+Speech(LSTM model):https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/audio_song_speech.ipynb   

Song only(LSTM model):https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/DL_project_audio_song.ipynb   

Speech only(LSTM model):https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/DL_project_audio_speech.ipynb  

LSTM model on embeds: https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/vggish_feature_to_my_model.ipynb  

Class model on Embeds: https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/ClassicModelOnEmbeds.ipynb  

Deep learning model on Embeds:https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/DeepModelOnEmbeds.ipynb  

Transfer learning VGGish: https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/TransferLearningVggish.ipynb  

3) Merge part:  
audio model and video model merge: https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/TwoInputOneModel.ipynb  

audio original feature and image features merge: https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/model_merge.ipynb  
