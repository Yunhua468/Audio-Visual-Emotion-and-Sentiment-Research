1. Everyone's script code work is under his/her name folder    
2. The 3 python files under Notebook folder is our best result notebooks  
3. The following is the summary of our work.

The following link is our final project notebooks:  
1) Video part best result: 
[Patrick_Jean-Baptiste/visual_pre-trained_model.ipynb](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/visual_pre-trained_model.ipynb)  
The notebook uses the scripts:  
[get_data.py](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Scripts/Patrick_Jean-Baptiste/get_data.py)  
[match_files.py](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Scripts/Patrick_Jean-Baptiste/match_files.py)

2) Audio part best result: [Yunhua/audio_song_speech.ipynb](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/audio_song_speech.ipynb)  

Our models:  
1) Video parts:   
* [Video preprocessing](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/video_preprocessing.ipynb)   

* [Face detection](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/face_detection.ipynb)  

* [Face extraction](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/face_extraction.ipynb)  

* [Emotion image collection](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/emotion_image_collection.ipynb)

* [Initial visual models](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/visual_models.ipynb)

[Visual emotion recognition](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/visual_emotion_recognition.ipynb)

* [Pre-trained VGG model](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Patrick_Jean-Baptiste/visual_pre-trained_model.ipynb)  

2) Audio part notebooks:   
* [Song+Speech(LSTM model)](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/audio_song_speech.ipynb)   

* [Song only(LSTM model)](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/DL_project_audio_song.ipynb)   

* [Speech only(LSTM model)](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/DL_project_audio_speech.ipynb)  

* [LSTM model on embeds](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/vggish_feature_to_my_model.ipynb)  

* [Class model on Embeds](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/ClassicModelOnEmbeds.ipynb)  

* [Deep learning model on Embeds](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/DeepModelOnEmbeds.ipynb)  

* [Transfer learning VGGish](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/TransferLearningVggish.ipynb)  

3) Merge part:  
* [audio model and video model merge](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/EnisBerk/TwoInputOneModel.ipynb)  

* [audio original feature and image features merge](https://github.com/Yunhua468/Audio-Visual-Emotion-and-Sentiment-Research/blob/master/Notebooks/Yunhua/model_merge.ipynb)  
