# Audio-Visual-Emotion-and-Sentiment-Research
Deep Neural Network and its application with TensorFlow project
Members:   
Audio parts: Enis Berk Ã‡oban and Yunhua Zhao   
             We use the audio-song, audio-speech, audio-song-speech files separately to train models to detect emotion.  
             Also we use different models(VGGish and LSTM) to do the emotion recognization.  
Video parts: Patrick Jean-Baptiste, Tianyu Gao

Period of our project:  
1) explore the dataset, Enis extractes audio from video, Yunhua decode the filenames, Patrick extract images from the video files
2) Yunhua use LSTM to train models on audio-song files to get the accuracy; then use same model to the audio-speech files to get the accuracy; then use same model to the audio-song-speech model.  
   Enis use VGGish to train the model on audio-song-speech files
3) Enis split the dataset into train, validation, test sets, and make a csv file, so that everyone could use the same train-val-test dataset to compare the final result.  
4) Patrick detect the face part from the image
5) Enis try Yunhua's model to the splited dataset, Yunhua tried Enis' model, Patrick train model on the split dataset to do the emotion classification  
6) Enis proves several organized files and leads us to move on
7) We make the PPT together.  

             


